# Epistemic Agent Runtime (EAR) Integration

Code Puppy now includes the **Epistemic Agent Runtime** â€” a structured methodology for building software through evidence-based reasoning.

> **The Core Insight:** Write down what you believe, how confident you are, and how you'd prove yourself wrong. That's it. Everything else â€” lenses, gates, the Ralph loop â€” is machinery to make that practice systematic and scalable.

## ğŸ§  What is EAR?

EAR provides a rigorous, 12-stage pipeline for going from idea â†’ validated specs â†’ working product:

```
Idea â†’ Epistemic State â†’ Lens Evaluation â†’ Gap Analysis â†’ Goal Emergence 
    â†’ MVP Planning â†’ Build Execution â†’ Improvement Loop
```

**Core Philosophy: Emergence-first â†’ Lens-driven â†’ Goal-earned â†’ Commit**

## ï¿½ The Sibling Folder Pattern

EAR uses a **read-only template + spawned project** architecture:

```
parent-folder/
â”œâ”€â”€ epistemic-project-template/    â† READ-ONLY reference (methodology)
â”‚   â”œâ”€â”€ CLAUDE.md                  â† Agent instructions
â”‚   â”œâ”€â”€ philosophy/                â† EAR philosophy
â”‚   â”œâ”€â”€ ear-runtime/               â† Python EAR library
â”‚   â”œâ”€â”€ process/                   â† Methodology docs
â”‚   â””â”€â”€ templates/                 â† Blank scaffolds
â”‚
â””â”€â”€ your-new-project/              â† WHERE WORK HAPPENS (spawned)
    â”œâ”€â”€ README.md
    â”œâ”€â”€ BUILD.md
    â”œâ”€â”€ epistemic/                 â† Your project's epistemic state
    â”œâ”€â”€ docs/                      â† Your analysis documents
    â”œâ”€â”€ specs/                     â† Your validated specifications
    â””â”€â”€ src/                       â† Your actual code
```

**Why this matters:**
- **Template stays pristine** â€” You never corrupt methodology with project-specific stuff
- **Agent can reference both** â€” Works in your project but can look back at template for philosophy
- **Reusable** â€” Next project? Same template, new sibling folder
- **State versioning** â€” Track how understanding evolves over time ("git for beliefs")

## ï¿½ğŸš€ Quick Start

### 1. Switch to the Epistemic Architect Agent

```
/agent epistemic-architect
```

### 2. Start an Epistemic Session

```
/epistemic start my-project
```

### 3. Describe Your Project

> "I want to build an API that helps developers track their technical debt..."

The agent will guide you through structured planning before any code is written.

## ğŸ“‹ The 12-Stage Pipeline

| Stage | Name | What Happens |
|-------|------|--------------|
| 0 | Philosophical Foundation | Internalize Ralph Loops and core principles |
| 1 | Epistemic State Creation | Surface assumptions, hypotheses, constraints |
| 2 | Lens Evaluation | Apply 7 expert perspectives |
| 3 | Gap Analysis | Identify CRITICAL/HIGH/MEDIUM/LOW gaps |
| 4 | Goal Emergence | Generate candidates, run through 6 gates |
| 5 | MVP Planning | Create minimal viable plan with rollback |
| 6 | Spec Generation | Generate full specs, readiness check |
| 7 | Build Execution | Phase â†’ Milestone â†’ Checkpoint â†’ Verify |
| 8 | Improvement Audit | Evidence â†’ Analysis â†’ Recommendation loop |
| 9 | Gap Re-Inspection | What new gaps emerged? Re-validate |
| 10 | Question Tracking | Update epistemic state, close hypotheses |
| 11 | Verification Audit | End-to-end check across all layers |
| 12 | Documentation Sync | Update all docs, then loop to Stage 8 |

## ğŸ” The 7 Expert Lenses

Each lens examines your project from a specific perspective:

| Lens | Question | Outputs |
|------|----------|---------|
| ğŸ§  **Philosophy** | What are we assuming? | Hidden assumptions, category errors |
| ğŸ“Š **Data Science** | Can we measure this? | Metrics plan, experiment design |
| ğŸ›¡ï¸ **Safety/Risk** | What could go wrong? | Risk flags, abuse vectors |
| ğŸ”· **Topology** | What's the structure? | Dependencies, phase transitions |
| âˆ‘ **Theoretical Math** | Is this consistent? | Minimal axioms, counterexamples |
| âš™ï¸ **Systems Engineering** | Can we build this? | Service boundaries, failure recovery |
| ğŸ‘¤ **Product/UX** | Does this help users? | Value hypotheses, MVP scope |

## âœ… The 6 Quality Gates

Goals must pass ALL gates before becoming actionable:

1. **ğŸ‘ï¸ Observables** â€” Does it have measurable outcomes?
2. **ğŸ§ª Testability** â€” Clear success/failure criteria?
3. **â†©ï¸ Reversibility** â€” Is there a rollback plan?
4. **ğŸ“ˆ Confidence** â€” Is confidence â‰¥ 0.6?
5. **ğŸ¤ Lens Agreement** â€” Do 3+ lenses approve?
6. **ğŸ“š Evidence Grounding** â€” Based on actual evidence?

## ğŸ”„ Ralph (Wiggum) Loops

The universal primitive for all epistemic work:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                     â”‚
    â–¼                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚OBSERVEâ”‚ â†’  â”‚ORIENT â”‚ â†’  â”‚DECIDE â”‚ â†’  â”‚ ACT   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜
    â–²                                     â”‚
    â”‚                                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Use `/ralph` to get guidance on each step.

### ğŸ¤ OODA-Driven Agent Delegation

The Epistemic Architect **orchestrates work** by delegating to specialist agents based on OODA phase and **workload type**:

**OBSERVE Phase** â†’ Architect uses own tools
- File exploration (`list_files`, `read_file`, `grep`)
- Project setup (`agent_run_shell_command`)
- Direct observation to build epistemic state

**ORIENT Phase** â†’ Delegate to REASONING workload specialists
- `invoke_agent("security-auditor", ...)` â€” Security analysis [REASONING]
- `invoke_agent("code-reviewer", ...)` â€” Code quality review [REASONING]
- `invoke_agent("qa-expert", ...)` â€” Test strategy [REASONING]
- `invoke_agent("shepherd", ...)` â€” Acceptance criteria review [REASONING]
- `invoke_agent("watchdog", ...)` â€” QA validation [REASONING]
- Multiple analyses run **in parallel** for efficiency

**DECIDE Phase** â†’ Use ORCHESTRATOR workload agents
- `invoke_agent("planning-agent", ...)` â€” Task breakdown [ORCHESTRATOR]
- `invoke_agent("pack-leader", ...)` â€” Multi-agent coordination [ORCHESTRATOR]
- `invoke_agent("helios", ...)` â€” Architecture design [ORCHESTRATOR]
- Architect synthesizes results and makes strategic decisions

**ACT Phase** â†’ Delegate to CODING/LIBRARIAN workload specialists
- `invoke_agent("python-programmer", ...)` â€” Python implementation [CODING]
- `invoke_agent("test-generator", ...)` â€” Test creation [CODING]
- `invoke_agent("doc-writer", ...)` â€” Documentation [LIBRARIAN]
- Parallel implementation by specialists

**Workload-Based Model Routing:**
- **ORCHESTRATOR**: Kimi K2.5 / Qwen3 â€” Complex reasoning, planning
- **REASONING**: DeepSeek R1 / GPT-5.2 â€” Analysis, code review
- **CODING**: Cerebras GLM 4.7 â€” Fast code generation
- **LIBRARIAN**: Haiku / Gemini Flash â€” Docs, context (cheap)

**Benefits:**
- âœ… Each agent uses optimal model based on workload type
- âœ… Parallel execution speeds up ORIENT and ACT phases
- âœ… Cost-efficient: expensive models only when needed
- âœ… Automatic failover via `RateLimitFailover` chains

## ğŸ“ Commands Reference

### Session Management

| Command | Description |
|---------|-------------|
| `/epistemic start <project>` | Start a new epistemic session |
| `/epistemic status` | Show current session status |
| `/epistemic stage` | Advance to next pipeline stage |
| `/epistemic stage <n>` | Jump to specific stage (0-12) |
| `/epistemic pause <reason>` | Pause with a reason |
| `/epistemic resume` | Resume paused session |
| `/epistemic save` | Save state to `epistemic/state.json` |
| `/epistemic load` | Load state from `epistemic/state.json` |
| `/epistemic end` | End the current session |

### Analysis Tools

| Command | Description |
|---------|-------------|
| `/epistemic gaps` | Show identified gaps by severity |
| `/epistemic assumptions` | Show recorded assumptions with confidence |
| `/lens <name>` | Apply a specific lens (philosophy, data, safety, etc.) |
| `/lens all` | Show all 7 lenses |
| `/ralph` | Show Ralph Loop structure |
| `/ralph <step>` | Focus on specific step (observe, orient, decide, act) |

## ğŸ“‚ Artifact Structure

When you scaffold an epistemic project, it creates:

```
project/
â”œâ”€â”€ README.md
â”œâ”€â”€ BUILD.md              â† The execution plan
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ epistemic/            â† Epistemic state (Stage 1)
â”‚   â”œâ”€â”€ state.json        â† Machine-readable state
â”‚   â”œâ”€â”€ assumptions.md    â† Documented assumptions
â”‚   â”œâ”€â”€ hypotheses.md     â† Testable hypotheses
â”‚   â”œâ”€â”€ constraints.md    â† Hard and soft constraints
â”‚   â””â”€â”€ evidence.md       â† Supporting evidence
â”œâ”€â”€ docs/                 â† Analysis documents (Stages 2-5)
â”‚   â”œâ”€â”€ lens-evaluation.md
â”‚   â”œâ”€â”€ gap-analysis.md
â”‚   â”œâ”€â”€ goals-and-gates.md
â”‚   â””â”€â”€ improvement-plan.md
â””â”€â”€ specs/                â† Specifications (Stage 6)
    â”œâ”€â”€ entities.md       â† Data model
    â”œâ”€â”€ personas.md       â† User personas
    â”œâ”€â”€ critical-flows.md â† Must-work user flows
    â”œâ”€â”€ metrics.md        â† Success metrics
    â””â”€â”€ trust-safety.md   â† Trust and safety policies
```

## â¸ï¸ When to Pause

The agent will pause and ask for human input when:

- ğŸ”´ CRITICAL gap found
- âŒ Goals fail gates
- âš ï¸ Readiness check fails
- ğŸ¤” Lenses strongly disagree
- ğŸ“‰ Confidence drops below 0.6
- ğŸ›¡ï¸ Safety lens raises risk flags
- âœ… After each major phase

## ğŸ’¡ Best Practices

1. **Ask "What would change my mind?"** for every assumption
2. **Quantify confidence** (0.0â€“1.0) to make beliefs explicit
3. **Name the lens** that surfaced each concern
4. **Track provenance** â€” every claim links to evidence
5. **Don't block on uncontrollables** â€” build measurement, not outcomes
6. **Small reversible steps** over big irreversible leaps

## ğŸ”— EAR Runtime Library

Code Puppy includes the full EAR Python library as a submodule at `code_puppy/epistemic/`. This provides:

- `ear.core` â€” State management, provenance, Ralph loops
- `ear.lenses` â€” All 7 expert lenses
- `ear.goals` â€” Goal candidates, gates, MVP planning
- `ear.sandbox` â€” Agent simulation and experimentation
- `ear.commitment` â€” Review gates, testing, rollback
- `ear.control` â€” Pause triggers, human-in-the-loop
- `ear.versioning` â€” State commits, branches, diffs

### Using EAR Programmatically

```python
from ear.core import EpistemicState, RalphLoop
from ear.lenses import create_default_registry
from ear.goals import GoalGenerator, GateKeeper

# Create epistemic state
state = EpistemicState()
state.add_assumption("Users want fast responses", confidence=0.8)

# Apply lenses
registry = create_default_registry()
outputs = registry.evaluate_all(state)

# Generate and validate goals
generator = GoalGenerator(state)
candidates = generator.generate_candidates()
gatekeeper = GateKeeper()
approved = gatekeeper.filter_passing(candidates, state)
```

## ğŸ“Š 4-Tier Adoption Model

EAR scales from solo projects to enterprise:

| Tier | Context | How EAR Helps |
|------|---------|---------------|
| **Tier 1** | Solo projects / prototypes | `ear init` scaffolds epistemic state. Run gap analysis to find what you haven't thought about. "What don't I know?" |
| **Tier 2** | Team projects / MVPs | Lenses in sprint planning. Track assumptions as first-class citizens with confidence scores. Test gates before shipping. |
| **Tier 3** | Production systems | Custom domain lenses (e.g., ComplianceLens). Automate epistemic state updates from A/B tests. Version your epistemic state. 600-line file cap. |
| **Tier 4** | Enterprise / multi-service | Per-service epistemic states. Cross-service dependency tracking via topology lens. Monthly "Epistemic Review" meetings. Governance gates before major decisions. |

## ğŸ­ Production Integration

For existing production systems, EAR works as an **overlay methodology** â€” not a rewrite:

### 1. Wrap Existing Decisions

Document what you currently assume is true about your product:

```python
state = EpistemicState()
state.add_assumption(
    content="Users prefer speed over accuracy",
    source="2024 user research",
    confidence=0.7
)
```

### 2. Run Lenses Against Current State

Let the 7 lenses find gaps you haven't considered:
- Safety risks not yet mitigated
- Metrics you aren't measuring
- Assumptions you haven't validated

### 3. Use Gates Before Major Changes

Before a feature launch, require the 6-gate protocol:
- Does it have measurable outcomes?
- Clear success criteria?
- Rollback plan?
- Sufficient confidence?
- Lens agreement?
- Evidence grounding?

### 4. Automate with CI

Run epistemic health checks in your pipeline:

```bash
# In CI pipeline
ear status --format json > epistemic-report.json
```

Publish epistemic health dashboards alongside your normal metrics.

### 5. Version Your Epistemic State

Track how your team's understanding evolves:

```bash
# State commits (like git for beliefs)
ear commit -m "Updated user retention hypothesis after A/B test"
ear diff HEAD~1  # See what changed
```

## ğŸ¤ Integration with Other Agents

The Epistemic Architect works well alongside other agents:

- Use **Pack Leader** for actual code execution after specs are validated
- Use **Pack Leader Cerebras Efficient** for token-conscious implementation
- Use **Code Reviewer** agents to validate implementation against specs
- Use **QA Expert** to test against the metrics defined in specs

Example workflow:
```
1. /agent epistemic-architect  â†’ Plan and validate
2. /epistemic save             â†’ Save the plan
3. /agent pack-leader          â†’ Execute the build
4. /agent code-reviewer        â†’ Review implementation
5. /agent epistemic-architect  â†’ Run improvement audit (Stage 8)
```

## ï¿½ Logfire Telemetry for EAR Loops

All EAR loop phases emit **real-time telemetry** to track confidence and completion:

| Event | Source | Purpose |
|-------|--------|---------|
| `ear_phase` | `ralph_loop.py` | Tracks OBSERVEâ†’ORIENTâ†’DECIDEâ†’ACT with confidence scores |

**Health Check Queries:** See [LOGFIRE-OBSERVABILITY.md](LOGFIRE-OBSERVABILITY.md) for SQL to verify:
- âœ… EAR loops complete >90% of the time
- âœ… Error rate <10%
- âœ… Average confidence scores by phase

## ï¿½ğŸ“š Further Reading

- [EAR Philosophy Documentation](code_puppy/epistemic/philosophy/project-plan.md)
- [Build Methodology](code_puppy/epistemic/process/build-methodology.md)
- [EAR Audit Loop](code_puppy/epistemic/process/ear-audit-loop.md)
- [Verification Checklist](code_puppy/epistemic/process/verification-checklist.md)

---

*The Epistemic Agent Runtime was developed based on principles from epistemology, systems engineering, and evidence-based software development.*
