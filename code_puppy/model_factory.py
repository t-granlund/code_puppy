import json
import logging
import os
import pathlib
from typing import Any, Dict

from anthropic import AsyncAnthropic
from openai import AsyncAzureOpenAI
from pydantic_ai.models.anthropic import AnthropicModel, AnthropicModelSettings
from pydantic_ai.models.openai import (
    OpenAIChatModel,
    OpenAIChatModelSettings,
    OpenAIResponsesModel,
)
from pydantic_ai.profiles import ModelProfile
from pydantic_ai.providers.anthropic import AnthropicProvider
from pydantic_ai.providers.cerebras import CerebrasProvider
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.providers.openrouter import OpenRouterProvider
from pydantic_ai.settings import ModelSettings

from code_puppy.gemini_model import GeminiModel
from code_puppy.messaging import emit_warning

from . import callbacks
from .claude_cache_client import ClaudeCacheAsyncClient, patch_anthropic_client_messages
from .config import EXTRA_MODELS_FILE, get_value
from .http_utils import create_async_client, get_cert_bundle_path, get_http2
from .round_robin_model import RoundRobinModel

logger = logging.getLogger(__name__)

# Registry for custom model provider classes from plugins
_CUSTOM_MODEL_PROVIDERS: Dict[str, type] = {}


def _load_plugin_model_providers():
    """Load custom model providers from plugins."""
    global _CUSTOM_MODEL_PROVIDERS
    try:
        from code_puppy.callbacks import on_register_model_providers
        results = on_register_model_providers()
        for result in results:
            if isinstance(result, dict):
                _CUSTOM_MODEL_PROVIDERS.update(result)
    except Exception:
        pass  # Don't break if plugins fail


# Load plugin model providers at module initialization
_load_plugin_model_providers()


def get_api_key(env_var_name: str) -> str | None:
    """Get an API key from config first, then fall back to environment variable.

    This allows users to set API keys via `/set KIMI_API_KEY=xxx` in addition to
    setting them as environment variables.

    Args:
        env_var_name: The name of the environment variable (e.g., "OPENAI_API_KEY")

    Returns:
        The API key value, or None if not found in either config or environment.
    """
    # First check config (case-insensitive key lookup)
    config_value = get_value(env_var_name.lower())
    if config_value:
        return config_value

    # Fall back to environment variable
    return os.environ.get(env_var_name)


def make_model_settings(
    model_name: str, max_tokens: int | None = None
) -> ModelSettings:
    """Create appropriate ModelSettings for a given model.

    This handles model-specific settings:
    - GPT-5 models: reasoning_effort and verbosity (non-codex only)
    - Claude/Anthropic models: extended_thinking and budget_tokens
    - Automatic max_tokens calculation based on model context length

    Args:
        model_name: The name of the model to create settings for.
        max_tokens: Optional max tokens limit. If None, automatically calculated
            as: max(2048, min(15% of context_length, 65536))

    Returns:
        Appropriate ModelSettings subclass instance for the model.
    """
    from code_puppy.config import (
        get_effective_model_settings,
        get_openai_reasoning_effort,
        get_openai_verbosity,
        model_supports_setting,
    )

    model_settings_dict: dict = {}

    # Calculate max_tokens if not explicitly provided
    if max_tokens is None:
        # Load model config to get context length
        try:
            models_config = ModelFactory.load_config()
            model_config = models_config.get(model_name, {})
            context_length = model_config.get("context_length", 128000)
        except Exception:
            # Fallback if config loading fails (e.g., in CI environments)
            context_length = 128000
        
        # CEREBRAS OPTIMIZATION: Use conservative max_tokens for Cerebras models
        # This prevents over-allocation which counts against TPM limits
        model_lower = model_name.lower()
        is_cerebras = "cerebras" in model_lower or "glm-4" in model_lower or "qwen" in model_lower
        
        if is_cerebras:
            # Cerebras: Default to 2000 tokens (tool calls are short)
            # Can be overridden dynamically based on task type
            max_tokens = 2000
        else:
            # min 2048, 15% of context, max 65536
            max_tokens = max(2048, min(int(0.15 * context_length), 65536))

    model_settings_dict["max_tokens"] = max_tokens
    effective_settings = get_effective_model_settings(model_name)
    model_settings_dict.update(effective_settings)

    # Default to clear_thinking=False for GLM-4.7 models (preserved thinking)
    if "glm-4.7" in model_name.lower():
        clear_thinking = effective_settings.get("clear_thinking", False)
        model_settings_dict["thinking"] = {
            "type": "enabled",
            "clear_thinking": clear_thinking,
        }

    model_settings: ModelSettings = ModelSettings(**model_settings_dict)

    if "gpt-5" in model_name:
        model_settings_dict["openai_reasoning_effort"] = get_openai_reasoning_effort()
        # Verbosity only applies to non-codex GPT-5 models (codex only supports "medium")
        if "codex" not in model_name:
            verbosity = get_openai_verbosity()
            model_settings_dict["extra_body"] = {"verbosity": verbosity}
        model_settings = OpenAIChatModelSettings(**model_settings_dict)
    elif model_name.startswith("claude-") or model_name.startswith("anthropic-"):
        # Handle Anthropic extended thinking settings
        # Remove top_p as Anthropic doesn't support it with extended thinking
        model_settings_dict.pop("top_p", None)

        # Claude extended thinking requires temperature=1.0 (API restriction)
        # Default to 1.0 if not explicitly set by user
        if model_settings_dict.get("temperature") is None:
            model_settings_dict["temperature"] = 1.0

        extended_thinking = effective_settings.get("extended_thinking", True)
        budget_tokens = effective_settings.get("budget_tokens", 10000)
        if extended_thinking and budget_tokens:
            model_settings_dict["anthropic_thinking"] = {
                "type": "enabled",
                "budget_tokens": budget_tokens,
            }
        model_settings = AnthropicModelSettings(**model_settings_dict)

    # Handle Gemini thinking models (Gemini-3)
    # Check if model supports thinking settings and apply defaults
    if model_supports_setting(model_name, "thinking_level"):
        # Apply defaults if not explicitly set by user
        # Default: thinking_enabled=True, thinking_level="low"
        if "thinking_enabled" not in model_settings_dict:
            model_settings_dict["thinking_enabled"] = True
        if "thinking_level" not in model_settings_dict:
            model_settings_dict["thinking_level"] = "low"
        # Recreate settings with Gemini thinking config
        model_settings = ModelSettings(**model_settings_dict)

    return model_settings


class ZaiChatModel(OpenAIChatModel):
    def _process_response(self, response):
        response.object = "chat.completion"
        return super()._process_response(response)


def get_custom_config(model_config):
    custom_config = model_config.get("custom_endpoint", {})
    if not custom_config:
        raise ValueError("Custom model requires 'custom_endpoint' configuration")

    url = custom_config.get("url")
    if not url:
        raise ValueError("Custom endpoint requires 'url' field")

    headers = {}
    for key, value in custom_config.get("headers", {}).items():
        if value.startswith("$"):
            env_var_name = value[1:]
            resolved_value = get_api_key(env_var_name)
            if resolved_value is None:
                emit_warning(
                    f"'{env_var_name}' is not set (check config or environment) for custom endpoint header '{key}'. Proceeding with empty value."
                )
                resolved_value = ""
            value = resolved_value
        elif "$" in value:
            tokens = value.split(" ")
            resolved_values = []
            for token in tokens:
                if token.startswith("$"):
                    env_var = token[1:]
                    resolved_value = get_api_key(env_var)
                    if resolved_value is None:
                        emit_warning(
                            f"'{env_var}' is not set (check config or environment) for custom endpoint header '{key}'. Proceeding with empty value."
                        )
                        resolved_values.append("")
                    else:
                        resolved_values.append(resolved_value)
                else:
                    resolved_values.append(token)
            value = " ".join(resolved_values)
        headers[key] = value
    api_key = None
    if "api_key" in custom_config:
        if custom_config["api_key"].startswith("$"):
            env_var_name = custom_config["api_key"][1:]
            api_key = get_api_key(env_var_name)
            if api_key is None:
                emit_warning(
                    f"API key '{env_var_name}' is not set (checked config and environment); proceeding without API key."
                )
        else:
            api_key = custom_config["api_key"]
    if "ca_certs_path" in custom_config:
        verify = custom_config["ca_certs_path"]
    else:
        verify = None
    return url, headers, verify, api_key


class ModelFactory:
    """A factory for creating and managing different AI models."""

    @staticmethod
    def load_config() -> Dict[str, Any]:
        load_model_config_callbacks = callbacks.get_callbacks("load_model_config")
        if len(load_model_config_callbacks) > 0:
            if len(load_model_config_callbacks) > 1:
                logging.getLogger(__name__).warning(
                    "Multiple load_model_config callbacks registered, using the first"
                )
            config = callbacks.on_load_model_config()[0]
        else:
            from code_puppy.config import MODELS_FILE

            with open(pathlib.Path(__file__).parent / "models.json", "r") as src:
                with open(pathlib.Path(MODELS_FILE), "w") as target:
                    target.write(src.read())

            with open(MODELS_FILE, "r") as f:
                config = json.load(f)

        # Import OAuth model file paths from main config
        from code_puppy.config import (
            ANTIGRAVITY_MODELS_FILE,
            CHATGPT_MODELS_FILE,
            CLAUDE_MODELS_FILE,
            GEMINI_MODELS_FILE,
        )

        # Build list of extra model sources
        extra_sources: list[tuple[pathlib.Path, str, bool]] = [
            (pathlib.Path(EXTRA_MODELS_FILE), "extra models", False),
            (pathlib.Path(CHATGPT_MODELS_FILE), "ChatGPT OAuth models", False),
            (pathlib.Path(CLAUDE_MODELS_FILE), "Claude Code OAuth models", True),
            (pathlib.Path(GEMINI_MODELS_FILE), "Gemini OAuth models", False),
            (pathlib.Path(ANTIGRAVITY_MODELS_FILE), "Antigravity OAuth models", False),
        ]

        for source_path, label, use_filtered in extra_sources:
            if not source_path.exists():
                continue
            try:
                # Use filtered loading for Claude Code OAuth models to show only latest versions
                if use_filtered:
                    try:
                        from code_puppy.plugins.claude_code_oauth.utils import (
                            load_claude_models_filtered,
                        )

                        extra_config = load_claude_models_filtered()
                    except ImportError:
                        # Plugin not available, fall back to standard JSON loading
                        logging.getLogger(__name__).debug(
                            f"claude_code_oauth plugin not available, loading {label} as plain JSON"
                        )
                        with open(source_path, "r") as f:
                            extra_config = json.load(f)
                else:
                    with open(source_path, "r") as f:
                        extra_config = json.load(f)
                config.update(extra_config)
            except json.JSONDecodeError as exc:
                logging.getLogger(__name__).warning(
                    f"Failed to load {label} config from {source_path}: Invalid JSON - {exc}"
                )
            except Exception as exc:
                logging.getLogger(__name__).warning(
                    f"Failed to load {label} config from {source_path}: {exc}"
                )

        # Let plugins add/override models via load_models_config hook
        try:
            from code_puppy.callbacks import on_load_models_config

            results = on_load_models_config()
            for result in results:
                if isinstance(result, dict):
                    config.update(result)  # Plugin models override built-in
        except Exception as exc:
            logging.getLogger(__name__).debug(
                f"Failed to load plugin models config: {exc}"
            )

        return config

    @staticmethod
    def get_model(model_name: str, config: Dict[str, Any]) -> Any:
        """Returns a configured model instance based on the provided name and config.

        API key validation happens naturally within each model type's initialization,
        which emits warnings and returns None if keys are missing.
        """
        model_config = config.get(model_name)
        if not model_config:
            raise ValueError(f"Model '{model_name}' not found in configuration.")

        model_type = model_config.get("type")

        # Check for plugin-registered model provider classes first
        if model_type in _CUSTOM_MODEL_PROVIDERS:
            provider_class = _CUSTOM_MODEL_PROVIDERS[model_type]
            try:
                return provider_class(model_name=model_name, model_config=model_config, config=config)
            except Exception as e:
                logger.error(f"Custom model provider '{model_type}' failed: {e}")
                return None

        if model_type == "gemini":
            api_key = get_api_key("GEMINI_API_KEY")
            if not api_key:
                emit_warning(
                    f"GEMINI_API_KEY is not set (check config or environment); skipping Gemini model '{model_config.get('name')}'."
                )
                return None

            model = GeminiModel(model_name=model_config["name"], api_key=api_key)
            return model

        elif model_type == "openai":
            api_key = get_api_key("OPENAI_API_KEY")
            if not api_key:
                emit_warning(
                    f"OPENAI_API_KEY is not set (check config or environment); skipping OpenAI model '{model_config.get('name')}'."
                )
                return None

            provider = OpenAIProvider(api_key=api_key)
            model = OpenAIChatModel(model_name=model_config["name"], provider=provider)
            if "codex" in model_name:
                model = OpenAIResponsesModel(
                    model_name=model_config["name"], provider=provider
                )
            setattr(model, "provider", provider)
            return model

        elif model_type == "anthropic":
            api_key = get_api_key("ANTHROPIC_API_KEY")
            if not api_key:
                emit_warning(
                    f"ANTHROPIC_API_KEY is not set (check config or environment); skipping Anthropic model '{model_config.get('name')}'."
                )
                return None

            # Use the same caching client as claude_code models
            verify = get_cert_bundle_path()
            http2_enabled = get_http2()

            client = ClaudeCacheAsyncClient(
                verify=verify,
                timeout=180,
                http2=http2_enabled,
            )

            # Check if interleaved thinking is enabled for this model
            # Only applies to Claude 4 models (Opus 4.5, Opus 4.1, Opus 4, Sonnet 4)
            from code_puppy.config import get_effective_model_settings

            effective_settings = get_effective_model_settings(model_name)
            interleaved_thinking = effective_settings.get("interleaved_thinking", False)

            default_headers = {}
            if interleaved_thinking:
                default_headers["anthropic-beta"] = "interleaved-thinking-2025-05-14"

            anthropic_client = AsyncAnthropic(
                api_key=api_key,
                http_client=client,
                default_headers=default_headers if default_headers else None,
            )

            # Ensure cache_control is injected at the Anthropic SDK layer
            patch_anthropic_client_messages(anthropic_client)

            provider = AnthropicProvider(anthropic_client=anthropic_client)
            return AnthropicModel(model_name=model_config["name"], provider=provider)

        elif model_type == "custom_anthropic":
            url, headers, verify, api_key = get_custom_config(model_config)
            if not api_key:
                emit_warning(
                    f"API key is not set for custom Anthropic endpoint; skipping model '{model_config.get('name')}'."
                )
                return None

            # Use the same caching client as claude_code models
            if verify is None:
                verify = get_cert_bundle_path()

            http2_enabled = get_http2()

            client = ClaudeCacheAsyncClient(
                headers=headers,
                verify=verify,
                timeout=180,
                http2=http2_enabled,
            )

            # Check if interleaved thinking is enabled for this model
            from code_puppy.config import get_effective_model_settings

            effective_settings = get_effective_model_settings(model_name)
            interleaved_thinking = effective_settings.get("interleaved_thinking", False)

            default_headers = {}
            if interleaved_thinking:
                default_headers["anthropic-beta"] = "interleaved-thinking-2025-05-14"

            anthropic_client = AsyncAnthropic(
                base_url=url,
                http_client=client,
                api_key=api_key,
                default_headers=default_headers if default_headers else None,
            )

            # Ensure cache_control is injected at the Anthropic SDK layer
            patch_anthropic_client_messages(anthropic_client)

            provider = AnthropicProvider(anthropic_client=anthropic_client)
            return AnthropicModel(model_name=model_config["name"], provider=provider)
        # NOTE: 'claude_code' model type is now handled by the claude_code_oauth plugin
        # via the register_model_type callback. See plugins/claude_code_oauth/register_callbacks.py

        elif model_type == "azure_openai":
            azure_endpoint_config = model_config.get("azure_endpoint")
            if not azure_endpoint_config:
                raise ValueError(
                    "Azure OpenAI model type requires 'azure_endpoint' in its configuration."
                )
            azure_endpoint = azure_endpoint_config
            if azure_endpoint_config.startswith("$"):
                azure_endpoint = get_api_key(azure_endpoint_config[1:])
            if not azure_endpoint:
                emit_warning(
                    f"Azure OpenAI endpoint '{azure_endpoint_config[1:] if azure_endpoint_config.startswith('$') else azure_endpoint_config}' not found (check config or environment); skipping model '{model_config.get('name')}'."
                )
                return None

            api_version_config = model_config.get("api_version")
            if not api_version_config:
                raise ValueError(
                    "Azure OpenAI model type requires 'api_version' in its configuration."
                )
            api_version = api_version_config
            if api_version_config.startswith("$"):
                api_version = get_api_key(api_version_config[1:])
            if not api_version:
                emit_warning(
                    f"Azure OpenAI API version '{api_version_config[1:] if api_version_config.startswith('$') else api_version_config}' not found (check config or environment); skipping model '{model_config.get('name')}'."
                )
                return None

            api_key_config = model_config.get("api_key")
            if not api_key_config:
                raise ValueError(
                    "Azure OpenAI model type requires 'api_key' in its configuration."
                )
            api_key = api_key_config
            if api_key_config.startswith("$"):
                api_key = get_api_key(api_key_config[1:])
            if not api_key:
                emit_warning(
                    f"Azure OpenAI API key '{api_key_config[1:] if api_key_config.startswith('$') else api_key_config}' not found (check config or environment); skipping model '{model_config.get('name')}'."
                )
                return None

            # Configure max_retries for the Azure client, defaulting if not specified in config
            azure_max_retries = model_config.get("max_retries", 2)

            azure_client = AsyncAzureOpenAI(
                azure_endpoint=azure_endpoint,
                api_version=api_version,
                api_key=api_key,
                max_retries=azure_max_retries,
            )
            provider = OpenAIProvider(openai_client=azure_client)
            model = OpenAIChatModel(model_name=model_config["name"], provider=provider)
            setattr(model, "provider", provider)
            return model

        elif model_type == "custom_openai":
            url, headers, verify, api_key = get_custom_config(model_config)
            client = create_async_client(headers=headers, verify=verify)
            provider_args = dict(
                base_url=url,
                http_client=client,
            )
            if api_key:
                provider_args["api_key"] = api_key
            provider = OpenAIProvider(**provider_args)
            model = OpenAIChatModel(model_name=model_config["name"], provider=provider)
            if model_name == "chatgpt-gpt-5-codex":
                model = OpenAIResponsesModel(model_config["name"], provider=provider)
            setattr(model, "provider", provider)
            return model
        elif model_type == "zai_coding":
            api_key = get_api_key("ZAI_API_KEY")
            if not api_key:
                emit_warning(
                    f"ZAI_API_KEY is not set (check config or environment); skipping ZAI coding model '{model_config.get('name')}'."
                )
                return None
            provider = OpenAIProvider(
                api_key=api_key,
                base_url="https://api.z.ai/api/coding/paas/v4",
            )
            zai_model = ZaiChatModel(
                model_name=model_config["name"],
                provider=provider,
            )
            setattr(zai_model, "provider", provider)
            return zai_model
        elif model_type == "zai_api":
            api_key = get_api_key("ZAI_API_KEY")
            if not api_key:
                emit_warning(
                    f"ZAI_API_KEY is not set (check config or environment); skipping ZAI API model '{model_config.get('name')}'."
                )
                return None
            provider = OpenAIProvider(
                api_key=api_key,
                base_url="https://api.z.ai/api/paas/v4/",
            )
            zai_model = ZaiChatModel(
                model_name=model_config["name"],
                provider=provider,
            )
            setattr(zai_model, "provider", provider)
            return zai_model
        # NOTE: 'antigravity' model type is now handled by the antigravity_oauth plugin
        # via the register_model_type callback. See plugins/antigravity_oauth/register_callbacks.py

        elif model_type == "custom_gemini":
            # Backwards compatibility: delegate to antigravity plugin if antigravity flag is set
            # New configs use type="antigravity" directly, but old configs may have
            # type="custom_gemini" with antigravity=True
            if model_config.get("antigravity"):
                # Find and call the antigravity handler from the plugin
                registered_handlers = callbacks.on_register_model_types()
                for handler_info in registered_handlers:
                    handlers = (
                        handler_info
                        if isinstance(handler_info, list)
                        else [handler_info]
                        if handler_info
                        else []
                    )
                    for handler_entry in handlers:
                        if (
                            isinstance(handler_entry, dict)
                            and handler_entry.get("type") == "antigravity"
                        ):
                            handler = handler_entry.get("handler")
                            if callable(handler):
                                try:
                                    return handler(model_name, model_config, config)
                                except Exception as e:
                                    logger.error(f"Antigravity handler failed: {e}")
                                    return None
                # If no antigravity handler found, warn and fall through
                emit_warning(
                    f"Model '{model_config.get('name')}' has antigravity=True but antigravity plugin not loaded."
                )
                return None

            url, headers, verify, api_key = get_custom_config(model_config)
            if not api_key:
                emit_warning(
                    f"API key is not set for custom Gemini endpoint; skipping model '{model_config.get('name')}'."
                )
                return None

            client = create_async_client(headers=headers, verify=verify)
            model = GeminiModel(
                model_name=model_config["name"],
                api_key=api_key,
                base_url=url,
                http_client=client,
            )
            return model
        elif model_type == "cerebras":

            class ZaiCerebrasProvider(CerebrasProvider):
                def model_profile(self, model_name: str) -> ModelProfile | None:
                    profile = super().model_profile(model_name)
                    if model_name.startswith("zai"):
                        from pydantic_ai.profiles.qwen import qwen_model_profile

                        profile = profile.update(qwen_model_profile("qwen-3-coder"))
                    return profile

            url, headers, verify, api_key = get_custom_config(model_config)
            if not api_key:
                emit_warning(
                    f"API key is not set for Cerebras endpoint; skipping model '{model_config.get('name')}'."
                )
                return None
            # Add Cerebras 3rd party integration header
            headers["X-Cerebras-3rd-Party-Integration"] = "code-puppy"
            # Pass "cerebras" so RetryingAsyncClient knows to ignore Cerebras's
            # absurdly aggressive Retry-After headers (they send 60s!)
            # Note: model_config["name"] is "zai-glm-4.7", not "cerebras"
            client = create_async_client(
                headers=headers, verify=verify, model_name="cerebras"
            )
            provider_args = dict(
                api_key=api_key,
                http_client=client,
            )
            provider = ZaiCerebrasProvider(**provider_args)

            model = OpenAIChatModel(model_name=model_config["name"], provider=provider)
            setattr(model, "provider", provider)
            return model

        elif model_type == "openrouter":
            # Get API key from config, which can be an environment variable reference or raw value
            api_key_config = model_config.get("api_key")
            api_key = None

            if api_key_config:
                if api_key_config.startswith("$"):
                    # It's an environment variable reference
                    env_var_name = api_key_config[1:]  # Remove the $ prefix
                    api_key = get_api_key(env_var_name)
                    if api_key is None:
                        emit_warning(
                            f"OpenRouter API key '{env_var_name}' not found (check config or environment); skipping model '{model_config.get('name')}'."
                        )
                        return None
                else:
                    # It's a raw API key value
                    api_key = api_key_config
            else:
                # No API key in config, try to get it from config or the default environment variable
                api_key = get_api_key("OPENROUTER_API_KEY")
                if api_key is None:
                    emit_warning(
                        f"OPENROUTER_API_KEY is not set (check config or environment); skipping OpenRouter model '{model_config.get('name')}'."
                    )
                    return None

            provider = OpenRouterProvider(api_key=api_key)

            model = OpenAIChatModel(model_name=model_config["name"], provider=provider)
            setattr(model, "provider", provider)
            return model

        elif model_type == "gemini_oauth":
            # Gemini OAuth models use the Code Assist API (cloudcode-pa.googleapis.com)
            # This is a different API than the standard Generative Language API
            try:
                # Try user plugin first, then built-in plugin
                try:
                    from gemini_oauth.config import GEMINI_OAUTH_CONFIG
                    from gemini_oauth.utils import (
                        get_project_id,
                        get_valid_access_token,
                    )
                except ImportError:
                    from code_puppy.plugins.gemini_oauth.config import (
                        GEMINI_OAUTH_CONFIG,
                    )
                    from code_puppy.plugins.gemini_oauth.utils import (
                        get_project_id,
                        get_valid_access_token,
                    )
            except ImportError as exc:
                emit_warning(
                    f"Gemini OAuth plugin not available; skipping model '{model_config.get('name')}'. "
                    f"Error: {exc}"
                )
                return None

            # Get a valid access token (refreshing if needed)
            access_token = get_valid_access_token()
            if not access_token:
                emit_warning(
                    f"Failed to get valid Gemini OAuth token; skipping model '{model_config.get('name')}'. "
                    "Run /gemini-auth to re-authenticate."
                )
                return None

            # Get project ID from stored tokens
            project_id = get_project_id()
            if not project_id:
                emit_warning(
                    f"No Code Assist project ID found; skipping model '{model_config.get('name')}'. "
                    "Run /gemini-auth to re-authenticate."
                )
                return None

            # Import the Code Assist model wrapper
            from code_puppy.gemini_code_assist import GeminiCodeAssistModel

            # Create the Code Assist model
            model = GeminiCodeAssistModel(
                model_name=model_config["name"],
                access_token=access_token,
                project_id=project_id,
                api_base_url=GEMINI_OAUTH_CONFIG["api_base_url"],
                api_version=GEMINI_OAUTH_CONFIG["api_version"],
            )
            return model

        # NOTE: 'chatgpt_oauth' model type is now handled by the chatgpt_oauth plugin
        # via the register_model_type callback. See plugins/chatgpt_oauth/register_callbacks.py

        elif model_type == "round_robin":
            # Get the list of model names to use in the round-robin
            model_names = model_config.get("models")
            if not model_names or not isinstance(model_names, list):
                raise ValueError(
                    f"Round-robin model '{model_name}' requires a 'models' list in its configuration."
                )

            # Get the rotate_every parameter (default: 1)
            rotate_every = model_config.get("rotate_every", 1)

            # Resolve each model name to an actual model instance
            models = []
            for name in model_names:
                # Recursively get each model using the factory
                model = ModelFactory.get_model(name, config)
                models.append(model)

            # Create and return the round-robin model
            return RoundRobinModel(*models, rotate_every=rotate_every)

        else:
            # Check for plugin-registered model type handlers
            registered_handlers = callbacks.on_register_model_types()
            for handler_info in registered_handlers:
                # Handler info can be a list of dicts or a single dict
                if isinstance(handler_info, list):
                    handlers = handler_info
                else:
                    handlers = [handler_info] if handler_info else []

                for handler_entry in handlers:
                    if not isinstance(handler_entry, dict):
                        continue
                    if handler_entry.get("type") == model_type:
                        handler = handler_entry.get("handler")
                        if callable(handler):
                            try:
                                return handler(model_name, model_config, config)
                            except Exception as e:
                                logger.error(
                                    f"Plugin handler for model type '{model_type}' failed: {e}"
                                )
                                return None

            raise ValueError(f"Unsupported model type: {model_type}")
